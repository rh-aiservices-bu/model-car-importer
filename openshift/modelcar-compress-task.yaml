apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: compress-model
spec:
  workspaces:
    - name: shared-workspace
  params:
    - name: COMPRESS_MODEL
      type: string
      description: "Whether to compress the model using llmcompressor (true/false)"
    - name: SKIP_TASK
      type: string
      description: "Name of this task"
    - name: SKIP_TASKS
      type: string
      description: "Comma-separated list of tasks to skip"
  stepTemplate:
    resources:
      limits:
        nvidia.com/gpu: 4
        memory: "24Gi"
        cpu: 1
      requests:
        nvidia.com/gpu: 4
        cpu: 1
        memory: "24Gi"
  steps:
    - name: compress
      image: quay.io/opendatahub/llmcompressor-workbench:main
      timeout: 4h
      script: |
        #!/bin/sh
        set -e
        if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
          echo "Skipping compress-model task"
          exit 0
        fi
        if [ "$(params.COMPRESS_MODEL)" = "true" ]; then
          echo "Compressing model using llmcompressor..."
          echo "Current directory: $(pwd)"
          echo "Listing model directory contents:"
          ls -la /workspace/shared-workspace/model
          
          cd /workspace/shared-workspace/model
          pip install git+https://github.com/vllm-project/llm-compressor.git@kylesayrs/use-memory-compression-pathways datasets transformers
          cp /workspace/compress.py .
          python compress.py
          echo "Model compression complete!"
        else
          echo "Skipping model compression..."
        fi
      volumeMounts:
        - name: compress-script
          mountPath: /workspace/compress.py
          subPath: compress.py
  volumes:
    - name: compress-script
      configMap:
        name: compress-script 