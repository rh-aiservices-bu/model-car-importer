apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: modelcar-pipeline
spec:
  params:
    - name: HUGGINGFACE_MODEL
      type: string
      description: "The Hugging Face model repository (e.g., ibm-granite/granite-3.2-2b-instruct)"
    - name: OCI_IMAGE
      type: string
      description: "The OCI image destination (e.g., quay.io/my-user/my-modelcar)"
    - name: HUGGINGFACE_ALLOW_PATTERNS
      type: string
      description: 'Optional array of file patterns to allow default: "*.safetensors", "*.json", "*.txt"'
      default: ""
    - name: COMPRESS_MODEL
      type: string
      description: "Whether to compress the model using llmcompressor (true/false)"
      default: "false"
    - name: MODEL_NAME
      type: string
      description: "Name of the model to register in the model registry"
    - name: MODEL_VERSION
      type: string
      description: "Version of the model to register"
      default: "1.0.0"
    - name: SKIP_TASKS
      type: string
      description: "Comma-separated list of tasks to skip (e.g., 'cleanup-workspace,pull-model-from-huggingface,compress-model,build-and-push-modelcar')"
      default: ""
    - name: MODEL_REGISTRY_URL
      type: string
      description: "URL of the model registry service"
    - name: DEPLOY_MODEL
      type: string
      description: "Whether to deploy the model as an InferenceService (true/false)"
      default: "false"
    - name: EVALUATE_MODEL
      type: string
      description: "Whether to evaluate the model (true/false)"
      default: "false"
    - name: TASKS
      type: string
      description: "Comma-separated list of evaluation tasks to run (e.g., 'arc_easy,hellaswag,winogrande')"
      default: "humaneval,mbpp"
    - name: GUIDELLM_EVALUATE_MODEL
      type: string
      description: "Whether to run GuideLLM performance evaluation (true/false)"
      default: "false"
    - name: MAX_MODEL_LEN
      type: string
      description: "Maximum model length for vLLM (default: 8192)"
      default: "8192"
  workspaces:
    - name: shared-workspace
    - name: quay-auth-workspace
  tasks:
    - name: cleanup-workspace
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: SKIP_TASK
          - name: SKIP_TASKS
        steps:
          - name: cleanup
            image: registry.access.redhat.com/ubi8/ubi-minimal:latest
            securityContext:
              runAsUser: 1001
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping cleanup-workspace task"
                exit 0
              fi
              echo "Cleaning up workspace..."
              rm -rf /workspace/shared-workspace/model
              echo "Workspace cleanup complete!"
              echo "Setting workspace permissions..."
              mkdir /workspace/shared-workspace/model
              ls -la /workspace/shared-workspace
              chown -R 1001:1001 /workspace/shared-workspace/model
              chmod -R 755 /workspace/shared-workspace/model
              echo "Workspace cleanup complete!"
      params:
        - name: SKIP_TASK
          value: "cleanup-workspace"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
    - name: pull-model-from-huggingface
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: HUGGINGFACE_MODEL
          - name: HUGGINGFACE_ALLOW_PATTERNS
            type: string
            default: ""
          - name: SKIP_TASK
          - name: SKIP_TASKS
        steps:
          - name: download-model
            image: quay.io/hayesphilip/huggingface-modelcar-builder:latest
            securityContext:
              runAsUser: 1001
            resources:
              limits:
                memory: "16Gi"
                cpu: "2"
              requests:
                memory: "16Gi"
                cpu: "2"
            env:
              - name: HUGGINGFACE_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: huggingface-secret
                    key: HUGGINGFACE_TOKEN
                    optional: true
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping pull-model-from-huggingface task"
                exit 0
              fi
              echo "Downloading model from Hugging Face..."
              mkdir -p /workspace/shared-workspace/model
              CMD="python download_model.py -m $(params.HUGGINGFACE_MODEL) -t /workspace/shared-workspace/model --token $HUGGINGFACE_TOKEN"
              if [ ! -z "$(params.HUGGINGFACE_ALLOW_PATTERNS)" ]; then
                CMD="$CMD --allow-patterns $(params.HUGGINGFACE_ALLOW_PATTERNS)"
              fi
              eval $CMD
              echo "Download complete!"
              if [ -d /workspace/shared-workspace/model/.cache ]; then
                echo "Removing cache"
                rm -r /workspace/shared-workspace/model/.cache
              fi
              
              echo "Model directory structure:"
              echo "------------------------"
              ls -la /workspace/shared-workspace/model 
              echo
              
              if [ -f /workspace/shared-workspace/model/config.json ]; then
                echo "Config.json contents:"
                echo "--------------------"
                cat /workspace/shared-workspace/model/config.json
              else
                echo "Warning: config.json not found in model directory"
              fi
      params:
        - name: HUGGINGFACE_MODEL
          value: $(params.HUGGINGFACE_MODEL)
        - name: SKIP_TASK
          value: "pull-model-from-huggingface"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - cleanup-workspace
    - name: compress-model
      taskRef:
        name: compress-model
      timeout: 2h
      when:
        - input: $(params.COMPRESS_MODEL)
          operator: in
          values: ["true"]
        - input: $(params.SKIP_TASKS)
          operator: notin
          values: ["compress-model"]
      params:
        - name: COMPRESS_MODEL
          value: $(params.COMPRESS_MODEL)
        - name: SKIP_TASK
          value: "compress-model"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - pull-model-from-huggingface
    - name: evaluate-model
      taskRef:
        name: evaluate-model
      timeout: 2h
      when:
        - input: $(params.EVALUATE_MODEL)
          operator: in
          values: ["true"]
        - input: $(params.SKIP_TASKS)
          operator: notin
          values: ["evaluate-model"]
      params:
        - name: EVALUATE_MODEL
          value: $(params.EVALUATE_MODEL)
        - name: SKIP_TASK
          value: "evaluate-model"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: TASKS
          value: $(params.TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - compress-model
    - name: guidellm-evaluate-model
      taskRef:
        name: guidellm-evaluate-model-containerized
      timeout: 2h
      when:
        - input: $(params.GUIDELLM_EVALUATE_MODEL)
          operator: in
          values: ["true"]
        - input: $(params.SKIP_TASKS)
          operator: notin
          values: ["guidellm-evaluate-model"]
      params:
        - name: GUIDELLM_EVALUATE_MODEL
          value: $(params.GUIDELLM_EVALUATE_MODEL)
        - name: SKIP_TASK
          value: "guidellm-evaluate-model"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - wait-for-service
    - name: build-and-push-modelcar
      taskSpec:
        workspaces:
          - name: shared-workspace
          - name: quay-auth-workspace
        params:
          - name: OCI_REGISTRY_SOURCE
          - name: OCI_REGISTRY_DESTINATION
          - name: MODEL_VERSION
          - name: SKIP_TASK
          - name: SKIP_TASKS
        steps:
          - name: build-modelcar
            image: quay.io/hayesphilip/modelcar-builder:latest
            securityContext:
              runAsUser: 1001
            script: |
              #!/bin/bash
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping build-and-push-modelcar task"
                exit 0
              fi
              echo "Setting up temporary workspace..."
              TEMP_DIR="/tmp/olot-workspace"
              mkdir -p $TEMP_DIR
              cd $TEMP_DIR
              echo "Cloning and Installing OLOT..."
              git clone https://github.com/containers/olot.git
              cd olot
              # Verify Poetry is available
              poetry --version
              make
              make install
              export PATH=$PATH:/usr/local/bin
              echo "OLOT installed successfully!"
              IMAGE_DIR=download
              MODEL_DIR=/workspace/shared-workspace/model
              echo "Downloading OCI image from $(params.OCI_REGISTRY_SOURCE)..."
              rm -rf $IMAGE_DIR
              skopeo copy --multi-arch all --remove-signatures \
                docker://$(params.OCI_REGISTRY_SOURCE) \
                oci:${IMAGE_DIR}:latest
              echo "Finding and appending model files to OCI image..."
              
              # Build list of files for olot command
              FILES=()
              for file in $(find $MODEL_DIR -type f); do
                if [[ "$file" == *"README.md" ]]; then
                  FILES+=("--modelcard" "$file")
                else
                  FILES+=("$file")
                fi
              done
              
              # Call olot once with all files
              echo "Adding files to OCI image..."
              poetry run olot $IMAGE_DIR "${FILES[@]}"
              
              echo "Pushing updated OCI image to $(params.OCI_REGISTRY_DESTINATION):$(params.MODEL_VERSION)..."
              # Convert image name to lowercase
              LOWER_IMAGE=$(echo "$(params.OCI_REGISTRY_DESTINATION)" | tr '[:upper:]' '[:lower:]')
              skopeo copy --multi-arch all \
                --authfile /workspace/quay-auth-workspace/.dockerconfigjson \
                oci:${IMAGE_DIR}:latest \
                docker://${LOWER_IMAGE}:$(params.MODEL_VERSION)
      params:
        - name: OCI_REGISTRY_SOURCE
          value: "registry.access.redhat.com/ubi9-micro@sha256:414cfa255ea10eaef4528a26d5618eb67cf487b635ee20f8f14b9317bfd6a4be"
        - name: OCI_REGISTRY_DESTINATION
          value: $(params.OCI_IMAGE)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: SKIP_TASK
          value: "build-and-push-modelcar"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
        - name: quay-auth-workspace
          workspace: quay-auth-workspace
      runAfter:
        - evaluate-model
    - name: register-with-registry
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: OCI_IMAGE
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: MODEL_REGISTRY_URL
        steps:
          - name: register-model
            image: quay.io/hayesphilip/modelcar-register:latest
            script: |
              #!/bin/bash
              set -e
              echo "Running registration script..."
              python /workspace/register.py
            volumeMounts:
              - name: register-script
                mountPath: /workspace/register.py
                subPath: register.py
            env:
              - name: MODEL_NAME
                value: $(params.MODEL_NAME)
              - name: MODEL_VERSION
                value: $(params.MODEL_VERSION)
              - name: OCI_IMAGE
                value: $(params.OCI_IMAGE)
              - name: SKIP_TASK
                value: $(params.SKIP_TASK)
              - name: SKIP_TASKS
                value: $(params.SKIP_TASKS)
              - name: MODEL_REGISTRY_URL
                value: $(params.MODEL_REGISTRY_URL)
              - name: CLUSTER_DOMAIN
                value: "cluster.local"
        volumes:
          - name: register-script
            configMap:
              name: register-script
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: OCI_IMAGE
          value: $(params.OCI_IMAGE)
        - name: SKIP_TASK
          value: "register-with-registry"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: MODEL_REGISTRY_URL
          value: $(params.MODEL_REGISTRY_URL)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - build-and-push-modelcar
    - name: deploy-model
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: OCI_IMAGE
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: DEPLOY_MODEL
          - name: MAX_MODEL_LEN
        steps:
          - name: check-deployment
            image: quay.io/openshift/origin-cli:latest
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping deploy-model task"
                exit 0
              fi
              if [ "$(params.DEPLOY_MODEL)" != "true" ]; then
                echo "Model deployment skipped (DEPLOY_MODEL=false)"
                exit 0
              fi
              echo "Preparing to deploy model..."
              # Read model version ID
              MODEL_VERSION_ID=$(cat /workspace/shared-workspace/model_version_id)
              if [ -z "$MODEL_VERSION_ID" ]; then
                echo "Error: Model version ID not found"
                exit 1
              fi
              # Create ServingRuntime manifest
              cat > /workspace/shared-workspace/servingruntime.yaml << EOF
              apiVersion: serving.kserve.io/v1alpha1
              kind: ServingRuntime
              metadata:
                annotations:
                  opendatahub.io/accelerator-name: large-gpu-card
                  opendatahub.io/apiProtocol: REST
                  opendatahub.io/hardware-profile-name: large-gpu-card-bna9k-serving
                  opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
                  opendatahub.io/template-display-name: vLLM NVIDIA GPU ServingRuntime for KServe
                  opendatahub.io/template-name: vllm-cuda-runtime
                  openshift.io/display-name: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                name: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                namespace: $(oc project -q)
                labels:
                  opendatahub.io/dashboard: 'true'
              spec:
                annotations:
                  prometheus.io/path: /metrics
                  prometheus.io/port: '8080'
                containers:
                  - name: kserve-container
                    image: 'quay.io/modh/vllm@sha256:4f550996130e7d16cacb24ca9a2865e7cf51eddaab014ceaf31a1ea6ef86d4ec'
                    command:
                      - python
                      - '-m'
                      - vllm.entrypoints.openai.api_server
                    args:
                      - '--port=8080'
                      - '--model=/mnt/models'
                      - '--served-model-name={{.Name}}'
                      - '--max-model-len=$(params.MAX_MODEL_LEN)'
                    env:
                      - name: HF_HOME
                        value: /tmp/hf_home
                    ports:
                      - containerPort: 8080
                        protocol: TCP
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: shm
                multiModel: false
                supportedModelFormats:
                  - autoSelect: true
                    name: vLLM
                volumes:
                  - emptyDir:
                      medium: Memory
                      sizeLimit: 2Gi
                    name: shm
                serviceAccountName: modelcar-pipeline
              EOF
              echo "Created ServingRuntime manifest"
              echo "Applying ServingRuntime..."
              oc apply -f /workspace/shared-workspace/servingruntime.yaml
              echo "ServingRuntime created"
              # Create InferenceService manifest
              cat > /workspace/shared-workspace/inferenceservice.yaml << EOF
              apiVersion: serving.kserve.io/v1beta1
              kind: InferenceService
              metadata:
                annotations:
                  openshift.io/display-name: $(params.MODEL_NAME) - $(params.MODEL_VERSION)
                  serving.knative.openshift.io/enablePassthrough: 'true'
                  serving.kserve.io/deploymentMode: RawDeployment
                  sidecar.istio.io/inject: 'true'
                  sidecar.istio.io/rewriteAppHTTPProbers: 'true'
                name: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                namespace: $(oc project -q)
                finalizers:
                  - odh.inferenceservice.finalizers
                  - inferenceservice.finalizers
                labels:
                  modelregistry.opendatahub.io/model-version-id: '${MODEL_VERSION_ID}'
                  modelregistry.opendatahub.io/name: registry
                  opendatahub.io/dashboard: 'true'
              spec:
                predictor:
                  serviceAccountName: modelcar-pipeline
                  maxReplicas: 1
                  minReplicas: 1
                  model:
                    modelFormat:
                      name: vLLM
                    name: ''
                    resources:
                      limits:
                        cpu: '2'
                        memory: 8Gi
                        nvidia.com/gpu: '1'
                      requests:
                        cpu: '1'
                        memory: 4Gi
                        nvidia.com/gpu: '1'
                    runtime: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                    storageUri: 'oci://$(echo $(params.OCI_IMAGE) | tr '[:upper:]' '[:lower:]'):$(params.MODEL_VERSION)'
                  tolerations:
                    - effect: NoSchedule
                      key: nvidia.com/gpu
                      operator: Exists
                  imagePullSecrets:
                    - name: quay-auth
                serviceAccountName: modelcar-pipeline
              EOF
              echo "Created InferenceService manifest"
              echo "Applying InferenceService..."
              oc apply -f /workspace/shared-workspace/inferenceservice.yaml
              echo "Model deployment initiated"
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: OCI_IMAGE
          value: $(params.OCI_IMAGE)
        - name: SKIP_TASK
          value: "deploy-model"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: DEPLOY_MODEL
          value: $(params.DEPLOY_MODEL)
        - name: MAX_MODEL_LEN
          value: $(params.MAX_MODEL_LEN)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - register-with-registry
    - name: wait-for-service
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: DEPLOY_MODEL
        steps:
          - name: wait-for-ready
            image: quay.io/openshift/origin-cli:latest
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping wait-for-service task"
                exit 0
              fi
              if [ "$(params.DEPLOY_MODEL)" != "true" ]; then
                echo "Model deployment skipped (DEPLOY_MODEL=false)"
                exit 0
              fi
              SERVICE_NAME=$(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
              echo "Waiting for service $SERVICE_NAME to be ready..."
              # Wait for the service to be ready
              while true; do
                STATUS=$(oc get inferenceservice $SERVICE_NAME -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
                if [ "$STATUS" = "True" ]; then
                  echo "Service is ready!"
                  break
                fi
                echo "Service not ready yet, waiting..."
                sleep 10
              done
              # Get the external URL
              EXTERNAL_URL=$(oc get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}')
              echo "Service is available at: $EXTERNAL_URL"
              # Add port 8080 for vLLM service
              EXTERNAL_URL_WITH_PORT="${EXTERNAL_URL}:8080"
              echo "Service URL with port: $EXTERNAL_URL_WITH_PORT"
              # Save the URL for future reference
              echo $EXTERNAL_URL_WITH_PORT > /workspace/shared-workspace/service_url
              echo "Service URL with port saved to workspace"
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: SKIP_TASK
          value: "wait-for-service"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: DEPLOY_MODEL
          value: $(params.DEPLOY_MODEL)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - deploy-model
    - name: deploy-anything-llm
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: DEPLOY_MODEL
        steps:
          - name: deploy-ui
            image: quay.io/openshift/origin-cli:latest
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping deploy-anything-llm task"
                exit 0
              fi
              if [ "$(params.DEPLOY_MODEL)" != "true" ]; then
                echo "Model deployment skipped (DEPLOY_MODEL=false)"
                exit 0
              fi
              echo "Deploying AnythingLLM UI..."
              # Read service URL
              SERVICE_URL=$(cat /workspace/shared-workspace/service_url)
              if [ -z "$SERVICE_URL" ]; then
                echo "Error: Service URL not found"
                exit 1
              fi
              echo "Using service URL: $SERVICE_URL"
              # Create AnythingLLM deployment
              cat > /workspace/shared-workspace/anything-llm.yaml << EOF
              apiVersion: apps/v1
              kind: Deployment
              metadata:
                name: anything-llm
                namespace: $(oc project -q)
              spec:
                replicas: 1
                selector:
                  matchLabels:
                    app: anything-llm
                template:
                  metadata:
                    labels:
                      app: anything-llm
                  spec:
                    containers:
                    - name: anything-llm
                      image: quay.io/rh-aiservices-bu/anythingllm-workbench:latest
                      ports:
                      - containerPort: 3001
                      env:
                      - name: STORAGE_DIR
                        value: "/app/server/storage"
                      - name: LLM_PROVIDER
                        value: "generic-openai"
                      - name: GENERIC_OPEN_AI_BASE_PATH
                        value: "${SERVICE_URL}/v1"
                      - name: GENERIC_OPEN_AI_API_KEY
                        value: ""
                      - name: GENERIC_OPEN_AI_MODEL_PREF
                        value: "$(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-')-$(echo $(params.MODEL_VERSION) | tr '.' '-')"
                      - name: GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT
                        value: "4096"
                      - name: MODEL_BASE_PATH
                        value: "${SERVICE_URL}"
                      - name: VECTOR_DB
                        value: "lancedb"
                      - name: EMBEDDING_ENGINE
                        value: "native"
                      - name: DISABLE_TELEMETRY
                        value: "true"
                      volumeMounts:
                      - name: storage
                        mountPath: /app/server/storage
                    volumes:
                    - name: storage
                      emptyDir: {}
              ---
              apiVersion: v1
              kind: Service
              metadata:
                name: anything-llm
                namespace: $(oc project -q)
              spec:
                selector:
                  app: anything-llm
                ports:
                - port: 80
                  targetPort: 3001
                type: ClusterIP
              ---
              apiVersion: route.openshift.io/v1
              kind: Route
              metadata:
                name: anything-llm
                namespace: $(oc project -q)
              spec:
                to:
                  kind: Service
                  name: anything-llm
                  weight: 100
                port:
                  targetPort: 3001
                tls:
                  termination: edge
                  insecureEdgeTerminationPolicy: Allow
                wildcardPolicy: None
              EOF
              echo "Created AnythingLLM manifests"
              echo "Applying AnythingLLM deployment..."
              oc apply -f /workspace/shared-workspace/anything-llm.yaml
              echo "AnythingLLM deployment initiated"
              # Wait for deployment to be ready
              echo "Waiting for AnythingLLM to be ready..."
              oc rollout status deployment/anything-llm
              # Get the route URL
              ROUTE_URL=$(oc get route anything-llm -o jsonpath='{.spec.host}')
              echo "AnythingLLM is available at: https://${ROUTE_URL}"
              # Save the URL for future reference
              echo "https://${ROUTE_URL}" > /workspace/shared-workspace/anything-llm-url
              echo "AnythingLLM URL saved to workspace"
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: SKIP_TASK
          value: "deploy-anything-llm"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: DEPLOY_MODEL
          value: $(params.DEPLOY_MODEL)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - wait-for-service
