apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: evaluate-model
spec:
  workspaces:
    - name: shared-workspace
  params:
    - name: EVALUATE_MODEL
      type: string
      description: "Whether to evaluate the model (true/false)"
      default: "false"
    - name: SKIP_TASK
      type: string
      description: "Name of this task"
    - name: SKIP_TASKS
      type: string
      description: "Comma-separated list of tasks to skip"
  stepTemplate:
    resources:
      limits:
        nvidia.com/gpu: 4
        memory: "24Gi"
        cpu: 1
      requests:
        nvidia.com/gpu: 4
        cpu: 1
        memory: "24Gi"
  steps:
    - name: evaluate
      image: quay.io/opendatahub/llmcompressor-workbench:main
      timeout: 2h
      env:
        # NCCL environment variables for multi-GPU container compatibility
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: NCCL_P2P_DISABLE
          value: "1"
        - name: NCCL_SHM_DISABLE
          value: "1"
        - name: NCCL_NET_GDR_DISABLE
          value: "1"
        - name: NCCL_SOCKET_IFNAME
          value: "lo"
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_TIMEOUT
          value: "3600"
        - name: NCCL_BUFFSIZE
          value: "1048576"
        - name: HF_ALLOW_CODE_EVAL
          value: "1"
      volumeMounts:
        - name: evaluate-script
          mountPath: /workspace/scripts/evaluate.py
          subPath: evaluate.py
      script: |
        #!/bin/sh
        set -e
        if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
          echo "Skipping evaluate-model task"
          exit 0
        fi
        if [ "$(params.EVALUATE_MODEL)" = "true" ]; then
          echo "Evaluating models using lm-evaluation-harness..."
          echo "Current directory: $(pwd)"
          
          # Install required packages
          pip install "vllm" 
          pip install "lm-eval[hf,openai]" --upgrade
          pip install "requests"
          
          # Run the comparison evaluation script
          python /workspace/scripts/evaluate.py \
            --model-dir /workspace/shared-workspace/model 
          
          echo "Model evaluation complete!"
        else
          echo "Skipping model evaluation..."
        fi
  volumes:
    - name: evaluate-script
      configMap:
        name: evaluate-script
        items:
          - key: evaluate.py
            path: evaluate.py 