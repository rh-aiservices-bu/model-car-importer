apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: evaluate-model
spec:
  workspaces:
    - name: shared-workspace
  params:
    - name: EVALUATE_MODEL
      type: string
      description: "Whether to evaluate the model (true/false)"
      default: "false"
    - name: SKIP_TASK
      type: string
      description: "Name of this task"
    - name: SKIP_TASKS
      type: string
      description: "Comma-separated list of tasks to skip"
    - name: MODEL_NAME
      type: string
      description: "Model name for the deployed service"
    - name: MODEL_VERSION
      type: string
      description: "Model version for the deployed service"
  stepTemplate:
    resources:
      limits:
        memory: "32Gi"
        cpu: 4
        nvidia.com/gpu: 1
      requests:
        cpu: 2
        memory: "16Gi"
        nvidia.com/gpu: 1
  steps:
    - name: evaluate
      image: quay.io/opendatahub/llmcompressor-workbench:main
      timeout: 2h
      env:
        - name: HF_ALLOW_CODE_EVAL
          value: "1"
      script: |
        #!/bin/sh
        set -e
        if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
          echo "Skipping evaluate-model task"
          exit 0
        fi
        if [ "$(params.EVALUATE_MODEL)" = "true" ]; then
          echo "üöÄ Evaluating model using local vLLM with lm-evaluation-harness..."
          echo "Current directory: $(pwd)"
          
          # Install required packages
          pip install "lm-eval[vllm]" --upgrade
          
          # Check for local model directory
          if [ -d "/workspace/shared-workspace/model" ]; then
            LOCAL_MODEL_DIR="/workspace/shared-workspace/model"
            echo "üìÅ Using compressed model directory: $LOCAL_MODEL_DIR"
          elif [ -d "/workspace/shared-workspace/model_original" ]; then
            LOCAL_MODEL_DIR="/workspace/shared-workspace/model_original"
            echo "üìÅ Using original model directory: $LOCAL_MODEL_DIR"
          else
            echo "‚ùå No local model directory found in workspace"
            echo "Workspace contents:"
            ls -la /workspace/shared-workspace/
            exit 1
          fi
          
          echo "üîß Running lm_eval with local vLLM..."
          echo "  Model Directory: $LOCAL_MODEL_DIR"
          
          # Set environment variables
          export TOKENIZERS_PARALLELISM=false
          export HF_HOME=/tmp/hf_cache
          
          # Run lm_eval with local vLLM
          lm_eval --model vllm \
            --model_args "pretrained=$LOCAL_MODEL_DIR,tensor_parallel_size=1,gpu_memory_utilization=0.8,max_model_len=16000" \
            --tasks humaneval,mbpp \
            --num_fewshot 3 \
            --batch_size 1 \
            --output_path /workspace/shared-workspace/evaluation_results \
            --log_samples \
            --confirm_run_unsafe_code
          
          echo "‚úÖ Model evaluation complete!"
          echo "üìã Results saved to: /workspace/shared-workspace/evaluation_results"
        else
          echo "Skipping model evaluation..."
        fi