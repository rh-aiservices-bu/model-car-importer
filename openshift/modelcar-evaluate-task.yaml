apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: evaluate-model
spec:
  workspaces:
    - name: shared-workspace
  params:
    - name: EVALUATE_MODEL
      type: string
      description: "Whether to evaluate the model (true/false)"
      default: "false"
    - name: SKIP_TASK
      type: string
      description: "Name of this task"
    - name: SKIP_TASKS
      type: string
      description: "Comma-separated list of tasks to skip"
    - name: EVALUATION_SCRIPT
      type: string
      description: "Name of the ConfigMap containing the evaluation script"
      default: "evaluate-script"
  stepTemplate:
    resources:
      limits:
        nvidia.com/gpu: 1
        memory: "24Gi"
        cpu: 1
      requests:
        nvidia.com/gpu: 1
        cpu: 1
        memory: "24Gi"
  steps:
    - name: evaluate
      image: quay.io/opendatahub/llmcompressor-workbench:main
      timeout: 2h
      volumeMounts:
        - name: evaluate-script
          mountPath: /workspace/scripts/evaluate.py
          subPath: evaluate.py
      script: |
        #!/bin/sh
        set -e
        if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
          echo "Skipping evaluate-model task"
          exit 0
        fi
        if [ "$(params.EVALUATE_MODEL)" = "true" ]; then
          echo "Evaluating models using lm-evaluation-harness..."
          echo "Current directory: $(pwd)"
          export HF_ALLOW_CODE_EVAL=1
          
          # Install required packages
          pip install "vllm" 
          pip install "lm-eval[hf,openai]" --upgrade
          
          # Run the evaluation script
          python /workspace/scripts/evaluate.py \
            --model-dir /workspace/shared-workspace/model 
          
          echo "Model evaluation complete!"
        else
          echo "Skipping model evaluation..."
        fi
  volumes:
    - name: evaluate-script
      configMap:
        name: $(params.EVALUATION_SCRIPT)
        items:
          - key: evaluate.py
            path: evaluate.py 