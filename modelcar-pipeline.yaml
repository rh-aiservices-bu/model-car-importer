apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: modelcar-pipeline
spec:
  params:
    - name: HUGGINGFACE_MODEL
      type: string
      description: "The Hugging Face model repository (e.g., ibm-granite/granite-3.2-2b-instruct)"
    - name: OCI_IMAGE
      type: string
      description: "The OCI image destination (e.g., quay.io/my-user/my-modelcar)"
    - name: HUGGINGFACE_ALLOW_PATTERNS
      type: string
      description: 'Optional array of file patterns to allow default: "*.safetensors", "*.json", "*.txt"'
      default: ""
    - name: COMPRESS_MODEL
      type: string
      description: "Whether to compress the model using llmcompressor (true/false)"
      default: "false"
    - name: MODEL_NAME
      type: string
      description: "Name of the model to register in the model registry"
    - name: MODEL_VERSION
      type: string
      description: "Version of the model to register"
      default: "1.0.0"
    - name: SKIP_TASKS
      type: string
      description: "Comma-separated list of tasks to skip (e.g., 'cleanup-workspace,pull-model-from-huggingface,compress-model,build-and-push-modelcar')"
      default: ""
    - name: MODEL_REGISTRY_URL
      type: string
      description: "URL of the model registry service"
    - name: DEPLOY_MODEL
      type: string
      description: "Whether to deploy the model as an InferenceService (true/false)"
      default: "false"
  workspaces:
    - name: shared-workspace
    - name: quay-auth-workspace
  tasks:
    - name: cleanup-workspace
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: SKIP_TASK
          - name: SKIP_TASKS
        steps:
          - name: cleanup
            image: registry.access.redhat.com/ubi8/ubi-minimal:latest
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping cleanup-workspace task"
                exit 0
              fi
              echo "Cleaning up workspace..."
              rm -rf /workspace/shared-workspace/*
              echo "Workspace cleanup complete!"
      params:
        - name: SKIP_TASK
          value: "cleanup-workspace"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
    - name: pull-model-from-huggingface
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: HUGGINGFACE_MODEL
          - name: HUGGINGFACE_ALLOW_PATTERNS
            type: string
            default: ""
          - name: SKIP_TASK
          - name: SKIP_TASKS
        steps:
          - name: download-model
            image: quay.io/hayesphilip/huggingface-modelcar-builder:latest
            env:
              - name: HUGGINGFACE_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: huggingface-secret
                    key: HUGGINGFACE_TOKEN
                    optional: true
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping pull-model-from-huggingface task"
                exit 0
              fi
              echo "Downloading model from Hugging Face..."
              mkdir -p /workspace/shared-workspace/model
              CMD="python download_model.py -m $(params.HUGGINGFACE_MODEL) -t /workspace/shared-workspace/model --token $HUGGINGFACE_TOKEN"
              if [ ! -z "$(params.HUGGINGFACE_ALLOW_PATTERNS)" ]; then
                CMD="$CMD --allow-patterns $(params.HUGGINGFACE_ALLOW_PATTERNS)"
              fi
              eval $CMD
              echo "Download complete!"
              if [ -d /workspace/shared-workspace/model/.cache ]; then
                echo "Removing cache"
                rm -r /workspace/shared-workspace/model/.cache
              fi
      params:
        - name: HUGGINGFACE_MODEL
          value: $(params.HUGGINGFACE_MODEL)
        - name: SKIP_TASK
          value: "pull-model-from-huggingface"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - cleanup-workspace
    - name: compress-model
      taskRef:
        name: compress-model
      timeout: 2h
      params:
        - name: COMPRESS_MODEL
          value: $(params.COMPRESS_MODEL)
        - name: SKIP_TASK
          value: "compress-model"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - pull-model-from-huggingface
    - name: build-and-push-modelcar
      taskSpec:
        workspaces:
          - name: shared-workspace
          - name: quay-auth-workspace
        params:
          - name: OCI_REGISTRY_SOURCE
          - name: OCI_REGISTRY_DESTINATION
          - name: MODEL_VERSION
          - name: SKIP_TASK
          - name: SKIP_TASKS
        steps:
          - name: build-modelcar
            image: quay.io/hayesphilip/modelcar-builder:latest
            imagePullPolicy: Always
            securityContext:
              runAsUser: 1001
            script: |
              #!/bin/bash
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping build-and-push-modelcar task"
                exit 0
              fi
              echo "Setting up temporary workspace..."
              TEMP_DIR="/tmp/olot-workspace"
              mkdir -p $TEMP_DIR
              cd $TEMP_DIR
              echo "Cloning and Installing OLOT..."
              git clone https://github.com/containers/olot.git
              cd olot
              # Verify Poetry is available
              poetry --version
              make
              make install
              export PATH=$PATH:/usr/local/bin
              echo "OLOT installed successfully!"
              IMAGE_DIR=download
              MODEL_DIR=/workspace/shared-workspace/model
              echo "Downloading OCI image from $(params.OCI_REGISTRY_SOURCE)..."
              rm -rf $IMAGE_DIR
              skopeo copy --multi-arch all --remove-signatures \
                docker://$(params.OCI_REGISTRY_SOURCE) \
                oci:${IMAGE_DIR}:latest
              echo "Finding and appending model files to OCI image..."
              find $MODEL_DIR -name "*" -print0 | while IFS= read -r -d '' file; do
                echo "Adding $file to OCI image..."
                poetry run olot $IMAGE_DIR "$file"
              done
              echo "Pushing updated OCI image to $(params.OCI_REGISTRY_DESTINATION):$(params.MODEL_VERSION)..."
              # Convert image name to lowercase
              LOWER_IMAGE=$(echo "$(params.OCI_REGISTRY_DESTINATION)" | tr '[:upper:]' '[:lower:]')
              skopeo copy --multi-arch all \
                --authfile /workspace/quay-auth-workspace/.dockerconfigjson \
                oci:${IMAGE_DIR}:latest \
                docker://${LOWER_IMAGE}:$(params.MODEL_VERSION)
      params:
        - name: OCI_REGISTRY_SOURCE
          value: "registry.access.redhat.com/ubi9-micro@sha256:414cfa255ea10eaef4528a26d5618eb67cf487b635ee20f8f14b9317bfd6a4be"
        - name: OCI_REGISTRY_DESTINATION
          value: $(params.OCI_IMAGE)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: SKIP_TASK
          value: "build-and-push-modelcar"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
        - name: quay-auth-workspace
          workspace: quay-auth-workspace
      runAfter:
        - compress-model
    - name: register-with-registry
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: OCI_IMAGE
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: MODEL_REGISTRY_URL
        steps:
          - name: register-model
            image: python:3.10-slim
            script: |
              #!/usr/bin/env python3
              import os
              import sys
              import subprocess
              import json

              print("Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)")
              if os.environ.get('SKIP_TASK') in os.environ.get('SKIP_TASKS', '').split(','):
                  print("Skipping register-with-registry task")
                  sys.exit(0)

              print("Installing model-registry package...")
              # Create user-specific pip directory
              pip_dir = '/tmp/pip'
              os.makedirs(pip_dir, exist_ok=True)
              
              # Install package
              subprocess.check_call([sys.executable, "-m", "pip", "install", "--target", pip_dir, "model-registry==0.2.15"])
              
              # Add pip directory to Python path
              sys.path.insert(0, pip_dir)
              
              # Now import the module
              from model_registry import ModelRegistry
              from model_registry.types import ModelArtifact, ModelVersion, RegisteredModel
              from model_registry.exceptions import StoreError

              # Get namespace from service account
              namespace_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
              with open(namespace_file_path, 'r') as namespace_file:
                  namespace = namespace_file.read().strip()
              
              # Get cluster domain from environment
              cluster_domain = os.environ.get('CLUSTER_DOMAIN', 'cluster.local')
              
              # Get server address from environment
              server_address = os.environ.get('MODEL_REGISTRY_URL')
              if not server_address:
                  print("Error: MODEL_REGISTRY_URL environment variable not set")
                  sys.exit(1)
              print(f"Server address: {server_address}")

              # Set token path in environment
              os.environ["KF_PIPELINES_SA_TOKEN_PATH"] = "/var/run/secrets/kubernetes.io/serviceaccount/token"
              print("Token path set in environment")

              # Initialize model registry client
              registry = ModelRegistry(
                  server_address=server_address,
                  port=443,
                  author="modelcar-pipeline",
                  is_secure=False
              )

              # Extract model name from Hugging Face model ID
              model_name = os.environ.get('MODEL_NAME').lower()
              model_version = os.environ.get('MODEL_VERSION')
              oci_image = "oci://" + os.environ.get('OCI_IMAGE').lower() + ":latest"

              try:
                  # Register model with metadata
                  registered_model = registry.register_model(
                      model_name,
                      oci_image,
                      version=model_version,
                      description=f"Model downloaded from Hugging Face and compressed using GPTQ",
                      model_format_name="safetensors",
                      model_format_version="1",
                      storage_key="modelcar-storage",
                      storage_path="/workspace/shared-workspace/model",
                      metadata={
                          "source": "huggingface",
                          "framework": "pytorch",
                          "compressed": True
                      }
                  )

                  print(f"Successfully registered model: {model_name} version {model_version}")
                  print(f"Model URI: {oci_image}")
                  print(f"Model details available at: https://rhods-dashboard-redhat-ods-applications.{cluster_domain}/modelRegistry/modelcar-pipeline-registry/registeredModels/1/versions/{registry.get_model_version(model_name, model_version).id}/details")

                  # Save model version ID for deployment task
                  model_version_id = registry.get_model_version(model_name, model_version).id
                  with open('/workspace/shared-workspace/model_version_id', 'w') as f:
                      f.write(str(model_version_id))

              except StoreError as e:
                  print(f"Model version already exists: {model_name} version {model_version}")
                  try:
                      # Get existing model version details
                      model_details = registry.get_model_version(model_name, model_version)
                      print(f"Existing model details:")
                      print(f"Model ID: {model_details.id}")
                      print(f"Model Name: {model_details.name}")
                      
                      # Save model version ID for deployment task
                      with open('/workspace/shared-workspace/model_version_id', 'w') as f:
                          f.write(str(model_details.id))
                      
                      print("Task completed successfully - using existing model version")
                      sys.exit(0)
                  except Exception as e:
                      print(f"Error getting existing model details: {str(e)}")
                      sys.exit(1)
              except Exception as e:
                  print(f"Error registering model: {str(e)}")
                  sys.exit(1)
            env:
              - name: MODEL_NAME
                value: $(params.MODEL_NAME)
              - name: MODEL_VERSION
                value: $(params.MODEL_VERSION)
              - name: OCI_IMAGE
                value: $(params.OCI_IMAGE)
              - name: SKIP_TASK
                value: $(params.SKIP_TASK)
              - name: SKIP_TASKS
                value: $(params.SKIP_TASKS)
              - name: MODEL_REGISTRY_URL
                value: $(params.MODEL_REGISTRY_URL)
              - name: CLUSTER_DOMAIN
                value: "cluster.local"
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: OCI_IMAGE
          value: $(params.OCI_IMAGE)
        - name: SKIP_TASK
          value: "register-with-registry"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: MODEL_REGISTRY_URL
          value: $(params.MODEL_REGISTRY_URL)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - build-and-push-modelcar
    - name: deploy-model
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: OCI_IMAGE
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: DEPLOY_MODEL
        steps:
          - name: check-deployment
            image: quay.io/openshift/origin-cli:latest
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping deploy-model task"
                exit 0
              fi
              if [ "$(params.DEPLOY_MODEL)" != "true" ]; then
                echo "Model deployment skipped (DEPLOY_MODEL=false)"
                exit 0
              fi
              echo "Preparing to deploy model..."
              # Read model version ID
              MODEL_VERSION_ID=$(cat /workspace/shared-workspace/model_version_id)
              if [ -z "$MODEL_VERSION_ID" ]; then
                echo "Error: Model version ID not found"
                exit 1
              fi
              # Create ServingRuntime manifest
              cat > /workspace/shared-workspace/servingruntime.yaml << EOF
              apiVersion: serving.kserve.io/v1alpha1
              kind: ServingRuntime
              metadata:
                annotations:
                  opendatahub.io/accelerator-name: large-gpu-card
                  opendatahub.io/apiProtocol: REST
                  opendatahub.io/hardware-profile-name: large-gpu-card-bna9k-serving
                  opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
                  opendatahub.io/template-display-name: vLLM NVIDIA GPU ServingRuntime for KServe
                  opendatahub.io/template-name: vllm-cuda-runtime
                  openshift.io/display-name: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                name: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                namespace: $(oc project -q)
                labels:
                  opendatahub.io/dashboard: 'true'
              spec:
                annotations:
                  prometheus.io/path: /metrics
                  prometheus.io/port: '8080'
                containers:
                  - name: kserve-container
                    image: 'quay.io/modh/vllm@sha256:4f550996130e7d16cacb24ca9a2865e7cf51eddaab014ceaf31a1ea6ef86d4ec'
                    command:
                      - python
                      - '-m'
                      - vllm.entrypoints.openai.api_server
                    args:
                      - '--port=8080'
                      - '--model=/mnt/models'
                      - '--served-model-name={{.Name}}'
                      - '--max-model-len=8192'
                    env:
                      - name: HF_HOME
                        value: /tmp/hf_home
                    ports:
                      - containerPort: 8080
                        protocol: TCP
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: shm
                multiModel: false
                supportedModelFormats:
                  - autoSelect: true
                    name: vLLM
                volumes:
                  - emptyDir:
                      medium: Memory
                      sizeLimit: 2Gi
                    name: shm
                serviceAccountName: modelcar-pipeline
              EOF
              echo "Created ServingRuntime manifest"
              echo "Applying ServingRuntime..."
              oc apply -f /workspace/shared-workspace/servingruntime.yaml
              echo "ServingRuntime created"
              # Create InferenceService manifest
              cat > /workspace/shared-workspace/inferenceservice.yaml << EOF
              apiVersion: serving.kserve.io/v1beta1
              kind: InferenceService
              metadata:
                annotations:
                  openshift.io/display-name: $(params.MODEL_NAME) - $(params.MODEL_VERSION)
                  serving.knative.openshift.io/enablePassthrough: 'true'
                  serving.kserve.io/deploymentMode: Serverless
                  sidecar.istio.io/inject: 'true'
                  sidecar.istio.io/rewriteAppHTTPProbers: 'true'
                name: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                namespace: $(oc project -q)
                finalizers:
                  - odh.inferenceservice.finalizers
                  - inferenceservice.finalizers
                labels:
                  modelregistry.opendatahub.io/model-version-id: '${MODEL_VERSION_ID}'
                  modelregistry.opendatahub.io/name: registry
                  opendatahub.io/dashboard: 'true'
              spec:
                predictor:
                  serviceAccountName: modelcar-pipeline
                  maxReplicas: 1
                  minReplicas: 1
                  model:
                    modelFormat:
                      name: vLLM
                    name: ''
                    resources:
                      limits:
                        cpu: '2'
                        memory: 8Gi
                        nvidia.com/gpu: '1'
                      requests:
                        cpu: '1'
                        memory: 4Gi
                        nvidia.com/gpu: '1'
                    runtime: $(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-' | sed 's/[^a-z0-9-]//g')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
                    storageUri: 'oci://$(echo $(params.OCI_IMAGE) | tr '[:upper:]' '[:lower:]'):$(params.MODEL_VERSION)'
                  tolerations:
                    - effect: NoSchedule
                      key: nvidia.com/gpu
                      operator: Exists
                  imagePullSecrets:
                    - name: quay-auth
                serviceAccountName: modelcar-pipeline
              EOF
              echo "Created InferenceService manifest"
              echo "Applying InferenceService..."
              oc apply -f /workspace/shared-workspace/inferenceservice.yaml
              echo "Model deployment initiated"
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: OCI_IMAGE
          value: $(params.OCI_IMAGE)
        - name: SKIP_TASK
          value: "deploy-model"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: DEPLOY_MODEL
          value: $(params.DEPLOY_MODEL)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - register-with-registry
    - name: wait-for-service
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: DEPLOY_MODEL
        steps:
          - name: wait-for-ready
            image: quay.io/openshift/origin-cli:latest
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping wait-for-service task"
                exit 0
              fi
              if [ "$(params.DEPLOY_MODEL)" != "true" ]; then
                echo "Model deployment skipped (DEPLOY_MODEL=false)"
                exit 0
              fi
              SERVICE_NAME=$(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-')-$(echo $(params.MODEL_VERSION) | tr '.' '-')
              echo "Waiting for service $SERVICE_NAME to be ready..."
              # Wait for the service to be ready
              while true; do
                STATUS=$(oc get inferenceservice $SERVICE_NAME -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
                if [ "$STATUS" = "True" ]; then
                  echo "Service is ready!"
                  break
                fi
                echo "Service not ready yet, waiting..."
                sleep 10
              done
              # Get the external URL
              EXTERNAL_URL=$(oc get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}')
              echo "Service is available at: $EXTERNAL_URL"
              # Save the URL for future reference
              echo $EXTERNAL_URL > /workspace/shared-workspace/service_url
              echo "Service URL saved to workspace"
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: SKIP_TASK
          value: "wait-for-service"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: DEPLOY_MODEL
          value: $(params.DEPLOY_MODEL)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - deploy-model
    - name: deploy-anything-llm
      taskSpec:
        workspaces:
          - name: shared-workspace
        params:
          - name: MODEL_NAME
          - name: MODEL_VERSION
          - name: SKIP_TASK
          - name: SKIP_TASKS
          - name: DEPLOY_MODEL
        steps:
          - name: deploy-ui
            image: quay.io/openshift/origin-cli:latest
            script: |
              #!/bin/sh
              set -e
              echo "Developed by the Red Hat AI Customer Adoption and Innovation team (CAI)"
              if [[ ",$(params.SKIP_TASKS)," == *",$(params.SKIP_TASK),"* ]]; then
                echo "Skipping deploy-anything-llm task"
                exit 0
              fi
              if [ "$(params.DEPLOY_MODEL)" != "true" ]; then
                echo "Model deployment skipped (DEPLOY_MODEL=false)"
                exit 0
              fi
              echo "Deploying AnythingLLM UI..."
              # Read service URL
              SERVICE_URL=$(cat /workspace/shared-workspace/service_url)
              if [ -z "$SERVICE_URL" ]; then
                echo "Error: Service URL not found"
                exit 1
              fi
              echo "Using service URL: $SERVICE_URL"
              # Create AnythingLLM deployment
              cat > /workspace/shared-workspace/anything-llm.yaml << EOF
              apiVersion: apps/v1
              kind: Deployment
              metadata:
                name: anything-llm
                namespace: $(oc project -q)
              spec:
                replicas: 1
                selector:
                  matchLabels:
                    app: anything-llm
                template:
                  metadata:
                    labels:
                      app: anything-llm
                  spec:
                    containers:
                    - name: anything-llm
                      image: quay.io/rh-aiservices-bu/anythingllm-workbench:latest
                      ports:
                      - containerPort: 3001
                      env:
                      - name: STORAGE_DIR
                        value: "/app/server/storage"
                      - name: LLM_PROVIDER
                        value: "generic-openai"
                      - name: GENERIC_OPEN_AI_BASE_PATH
                        value: "${SERVICE_URL}/v1"
                      - name: GENERIC_OPEN_AI_API_KEY
                        value: ""
                      - name: GENERIC_OPEN_AI_MODEL_PREF
                        value: "$(echo $(params.MODEL_NAME) | tr '[:upper:]' '[:lower:]' | tr '/' '-' | tr '.' '-')-$(echo $(params.MODEL_VERSION) | tr '.' '-')"
                      - name: GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT
                        value: "4096"
                      - name: MODEL_BASE_PATH
                        value: "${SERVICE_URL}"
                      - name: VECTOR_DB
                        value: "lancedb"
                      - name: EMBEDDING_ENGINE
                        value: "native"
                      - name: DISABLE_TELEMETRY
                        value: "true"
                      volumeMounts:
                      - name: storage
                        mountPath: /app/server/storage
                    volumes:
                    - name: storage
                      emptyDir: {}
              ---
              apiVersion: v1
              kind: Service
              metadata:
                name: anything-llm
                namespace: $(oc project -q)
              spec:
                selector:
                  app: anything-llm
                ports:
                - port: 80
                  targetPort: 3001
                type: ClusterIP
              ---
              apiVersion: route.openshift.io/v1
              kind: Route
              metadata:
                name: anything-llm
                namespace: $(oc project -q)
              spec:
                to:
                  kind: Service
                  name: anything-llm
                  weight: 100
                port:
                  targetPort: 3001
                tls:
                  termination: edge
                  insecureEdgeTerminationPolicy: Allow
                wildcardPolicy: None
              EOF
              echo "Created AnythingLLM manifests"
              echo "Applying AnythingLLM deployment..."
              oc apply -f /workspace/shared-workspace/anything-llm.yaml
              echo "AnythingLLM deployment initiated"
              # Wait for deployment to be ready
              echo "Waiting for AnythingLLM to be ready..."
              oc rollout status deployment/anything-llm
              # Get the route URL
              ROUTE_URL=$(oc get route anything-llm -o jsonpath='{.spec.host}')
              echo "AnythingLLM is available at: https://${ROUTE_URL}"
              # Save the URL for future reference
              echo "https://${ROUTE_URL}" > /workspace/shared-workspace/anything-llm-url
              echo "AnythingLLM URL saved to workspace"
      params:
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: MODEL_VERSION
          value: $(params.MODEL_VERSION)
        - name: SKIP_TASK
          value: "deploy-anything-llm"
        - name: SKIP_TASKS
          value: $(params.SKIP_TASKS)
        - name: DEPLOY_MODEL
          value: $(params.DEPLOY_MODEL)
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
      runAfter:
        - wait-for-service
